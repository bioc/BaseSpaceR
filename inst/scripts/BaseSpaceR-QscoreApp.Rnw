%\VignetteIndexEntry{BaseSpaceR}
%\VignetteDepends{BaseSpaceR, ShortRead}
%\VignetteKeywords{BaseSpace}
%\VignettePackage{BaseSpaceR}

\documentclass[a4paper, oneside, 10pt]{article}

\usepackage[pdftex]{hyperref}
\usepackage{calc}
\usepackage{sectsty}
\usepackage{caption}
\usepackage{natbib}
\renewcommand{\captionfont}{\it\sffamily}
\renewcommand{\captionlabelfont}{\bf\sffamily}
\allsectionsfont{\sffamily}

% page style %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[a4paper, left=25mm, right=20mm, top=20mm, bottom=25mm, nohead]{geometry}
\setlength{\parskip}{1.5ex}
\setlength{\parindent}{0cm}
\pagestyle{empty}


\usepackage{Sweave}
\SweaveOpts{prefix.string = BaseSpaceR}


\title{\vspace*{-6ex} Sample App - Mean Q-score per cycle}
\author{Adrian Alexa}
\date{\today \\%
  \texttt{aalexa@illumina.com}}

\begin{document}
\maketitle

%%\newpage
\tableofcontents
\newpage

<<echo = FALSE>>=
options(width = 95)

getQscoreCounts <- function(fIn, maxS = 42L) {
  ## read the fastq file and keep the qualities 
  r <- quality(quality(readFastq(fIn, withIds = FALSE)))
  r_row <- width(r)[[1L]]
  r_col <- length(r)

  ## transorm the qualities to integers from 2 to maxS
  x <- as.integer(unlist(r, use.names = FALSE)) - 33L
  dim(x) <- c(r_row, r_col)

  ## tabulate each row in the matrix
  qtab <- matrix(0L, nrow = maxS, ncol = r_row,
                 dimnames = list(paste0("Q", seq_len(maxS)), NULL))
  for(i in seq_len(r_row)) 
    qtab[, i] <- tabulate(x[i, ], nbins = maxS)
  
  return(qtab)
}

getQscoreStats <- function(x) {
  nr <- nrow(x)
  nc <- ncol(x)
  qstat <- matrix(0L, nrow = nc, ncol = 4L,
                  dimnames = list(NULL, c("5%", "median", "95%", "mean")))
  rleval <- seq_len(nr)
  for(i in seq_len(nc)) {
    r <- Rle(values = rleval, lengths = x[, i])
    qstat[i, ] <- c(quantile(r, probs = c(0.05, .5, 0.95)), mean(r))
  }

  return(qstat)
}
@ 
    


\section{Introduction}
\label{sec:Intro}

This document describes a simple working session using {\tt BaseSpaceR}. 
For more details on the functions used throughout this document please see the 
package main vignette - {\bf BaseSpaceR.pdf}. 

A typical BaseSpace session can be divided into the following steps:
\begin{itemize}
\item Client authentication
\item Data retrieval and access
\item Data processing
\item Uploading results back to BaseSpace
\end{itemize}

In this sample session we show how to explore the project and sample data, select the
FASTQ file(s) associated with the chosen sample, compute and plot Q-score statistics 
at each cycle, create an {\tt AppResults} instance and link the generated figure to it. 

<<>>=
library(BaseSpaceR)
@ 


\section{Authentication}
\label{sec:auth}

The first step is the authentication of the client application with the BaseSpace REST
server.  The authentication and communication between the client and the server are handled
by an {\tt AppAuth} instance. There are two ways to create an {\tt AppAuth} instance.
The simplest is to provide a pre-generated access token, see the help page for {\tt AppAuth}
and the package main vignette for more details on this process.
We further assume the user has access to such a token, stored as a character string 
in {\tt app\_access\_token} variable. The communication handler can be created as follows:

<<>>=
aAuth <- AppAuth(access_token = app_access_token)
aAuth
@ 

If the access token is valid then the connection with the server is established and 
we can see it by printing the {\tt AppAuth} object (Authorized: TRUE).

One can also use the OAuth v2 authentication process, but in this case direct user 
interaction is required. Authentication through the OAuth v2 process is out of the scope
of this document and it will not be shown here.

Please note that for the purpose of this document one needs an access token with the 
following scope: {\tt create~projects,~write~global}. 


\section{Accessing the data}
\label{sec:getdata}

Now that our client App is authenticated with the server we can start exploring the
data. One of the first steps is to inspect the resources available to the user under
the current scope (associated with the access token).  

\subsection{Browsing available projects}
\label{subsec:browse}

Let's assume we need to browse all the projects accessible to the user. 
We can do this using the {\tt listProjects()} method.

<<>>=
myProj <- listProjects(aAuth)
myProj
@ 

For this instance there are $5$ projects available to the user. From the returned 
object we can get the names and IDs of the available projects.

<<>>=
data.frame(Name = Name(myProj), Id = Id(myProj))
@ 

For each available project we can get more detailed information. Let's assume we are
interested in project {\it BaseSpaceDemo} (with {\tt Id = 2}) for which we want
to analyze the Q-score distribution of the reads. We can select it using the
{\tt Projects()} method and the ID of the project(s) of interest.

<<>>=
selProj <- Projects(aAuth, id = 2, simplify = TRUE)
selProj
@ 


\subsection{Selecting samples}
\label{subsec:browsesampl}

Once we have decided the project we want to work on, we can browse the samples associated
with it. We can list all the samples or we can limit the search to a specific number
of samples. This can be achieved using the {\tt Limit} query parameter.

<<>>=
sampl <- listSamples(selProj, Limit = 1)
sampl
@ 

The same result can be achieved by querying directly the {\tt aAuth} handler and 
specifying the project ID.

<<>>=
listSamples(aAuth, projectId = Id(selProj), Limit = 1)
@ 

There are a total of $\Sexpr{TotalCount(sampl)}$ samples associated with this
project, and the listed sample (given that we restricted the result to one sample)
has the ID {\tt \Sexpr{Id(sampl)}}. We can get more information on this sample by
calling the {\tt Samples()} method with the {\tt aAuth} handler and the sample ID as
argument or just by calling it using the {\tt sampl} object.

<<>>=
inSample <- Samples(aAuth, id = Id(sampl), simplify = TRUE)
identical(inSample, Samples(sampl, simplify = TRUE))
inSample
@

As we can see, the {\tt inSample} object contains detailed information on the 
selected sample. For example, the {\tt NumReadsRaw} entry contains the number of 
read pairs available in the chosen sample. We can access each entry in the Samples
resource using the {\tt '\$'} operator. Please note that the access operator 
{\tt '\$'}, works for every {\tt Response} object.

<<>>=
inSample$NumReadsRaw
@ 


\subsection{Selecting and accessing files}
\label{subsec:files}

We can now access the files linked to the selected sample. To browse the files we use
the {\tt listFiles()} function. We are interested in the FASTQ files.

<<>>=
f <- listFiles(inSample, Extensions = ".gz")
length(f)
f
@ 

The R API is design such that the user can use any query parameters implemented by
the REST API. Here we selected just the *.gz files, which in the context of the
{\tt Samples} resource these are only FASTQ files. 
The {\tt Path} element gives us the relative location of the file(s). We can use this
to uniquely identify the files.

<<>>=
f$Path # full location of the files
@ 

We further select the fastq files that contain only R1 or R2 in their name.
<<>>=
idx <- grep("_R(1|2)_", Name(f))
@ 

We can therefore download the selected file(s) to a local directory preserving
the path information.

<<>>=
outDir <- paste("Sample", Id(inSample), sep = "_")
floc <- file.path(outDir, f$Path[idx])
names(floc) <- basename(floc)
floc
@


Next we download the selected file(s) to the {\tt outDir} (this might take a while, 
depending on the number of files selected and the speed of the connection).
Method {\tt getFiles()} is used for downloading the data stream. 
<<eval = FALSE>>=
getFiles(aAuth, id = Id(f)[idx], destDir = outDir, verbose = FALSE)
@ 

<<echo = FALSE>>=
## a bit of caching
if(any(i <- file.exists(floc))) {
  ## check the file size for the ones already downloaded 
  idx <- c(idx[!i], idx[i][file.info(floc[i])$size != f$Size[idx[i]]])
}

## download the missing files
if(length(idx))
  getFiles(aAuth, id = Id(f)[idx], destDir = outDir)

rm(i); idx <- grep("_R(1|2)_", Name(f))
@ 

We can quickly check if the files were downloaded:
<<>>=
file.exists(floc)
@ 



\section{Data crunching}
\label{sec:crunching}

To compute the average Q-scores at each cycle we need to read the FASTQ file in R and
extract the base qualities for each read. The Bioconductor {\tt ShortRead} library
offers tools for this. The functions used in this computation, {\tt getQscoreCounts()}
and {\tt getQscoreStats()} are shown in Section~\ref{sec:functions}.

<<>>=
library(ShortRead)
qtab <- lapply(floc, getQscoreCounts)
@ 

One can use the {\tt parallel} package and the {\tt mclapply} function to compute
{\tt qtab}. This is an ideal scenario for batch processing since we perform 
the computation on each FASTQ file separately. 

<<eval = FALSE>>=
library(parallel)
qtab <- mclapply(floc, getQscoreCounts, mc.cores = length(floc))
@ 

If the read pairs are spread across multiple FASTQ files, we have to aggregate the
counts for both R1 and R2.

<<>>=
idxR1 <- grep("_R1_", names(floc), fixed = TRUE)
idxR2 <- grep("_R2_", names(floc), fixed = TRUE)
if(length(idxR1) != length(idxR2))
  stop("Missing files for R1 or R2")

x <- getQscoreStats(cbind(Reduce("+", qtab[idxR1]), Reduce("+", qtab[idxR2])))
@ 

We can quickly inspect the computed statistics:
<<>>=
head(x)
@ 


\subsection{Plotting Q-score statistics}

We can now generate a simple plot summarizing the Q-score distribution. We save the 
graph into a local .png file such that we can later upload this file to BaseSpace.

<<echo = FALSE>>=
ylim <- range(x) + c(-2L, 2L)
gfile <- file.path(tempdir(), "Qscore_per_cycle.png")

png(file = gfile, width = 1000, height = 500, bg = "transparent")
plot(x = seq_len(nrow(x)), type = "n", ylim = ylim,
     xlab = "Cycle", ylab = "Q-score",
     main = "Q-scores statistics")

sx <- apply(x[, c("5%", "95%")], 2, function(x) smooth.spline(x)$y)
sx[, "95%"] <- pmax(sx[, "95%"], x[, "median"])
polygon(c(1L:nrow(x), nrow(x):1L), c(sx[, "95%"], rev(sx[, "5%"])),
        col = "#CCEBC580", border = NA)
matpoints(sx, type = "l", lwd = .5, lty = 2, col = "black")
lines(x[, "mean"], lwd = 2, col = "red")
lines(x[, "median"], lwd = 2, col = "black")

legend("bottomleft", col = c("black", "red", "#CCEBC580"), 
       lwd = 10, inset = c(.05, .01), cex = 1.2, bty = "n",
       legend = c("Median", "Mean", "5% - 95%"))
dev.off()
@ 

\begin{figure}[!ht]
  \centering 
  \includegraphics[width=1.05\linewidth]{\Sexpr{gfile}}
\caption{Mean Q-score at each cycle. The shaded region gives the $5\%$ and $95\%$ bands.}
\label{fig:qscoreDist}
\end{figure}



\section{Storing the results}
\label{sec:results}

In this section we show how to store the analysis results back in BaseSpace.
In this example we'll just keep the PNG figure, but we could store any type of
file/data if necessary. 

To do this we first need to create a new {\tt AppResults} instance (possibly under the
current project if we have permission to add results to it). Here we assume that we 
don't have permission to modify the project from where we read the samples. So we'll
have to create a new project, associate the samples used with it and upload the 
analysis results.

\subsection{Creating a new project}
\label{subsec:newproject}

To create a new project we use the {\tt createProject()} method. In this process it is
a good practice to check whether a project with the same name already exists.

<<>>=
pname <- "Test Apps"

## shows the name of all projects the user can see 
Name(myProj)

idx <- which(Name(myProj) %in% pname)
projId <- if(length(idx) == 0L) {
  Id(createProject(aAuth, name = pname))
} else {
  Id(myProj)[idx]
}
@ 


We can take a closer look at the (newly created) project:
<<>>=
newProj <- Projects(aAuth, id = projId, simplify = TRUE)
newProj
@ 

If this project was created before, most likely there are {\tt AppResults} associated 
with it. We can browse the existing results within this project using the 
{\tt listAppResults()} method.


<<>>=
listAppResults(newProj)
@ 

\subsection{Creating a new AppResult}
\label{subsec:newappresult}

We next need to create a new {\tt AppResults} instance which will hold our analysis data.
To do this we create a JSON object with a well defined set of fields
(see {\it Writing back to BaseSpace} for more details).

<<>>=
ar <- list(Name = "Q-score dsitribution",
           Description = "Simple stats on the Q-score at each cycle.",
           "References" = list(Rel = "using",
             HrefContent = Href(inSample)))

cat(toJSON(ar, pretty = TRUE), "\n")
@ 

We are now ready to create the {\tt AppResults} instance.

<<>>=
newResult <- createAppResults(newProj, value = toJSON(ar))
@ 

If the user doesn't have write access to the current project, then the
{\tt newResult} object will be {\tt NULL} and an error message from the REST server
will be shown to the user. In this case we can launch the OAuth2 process with the 
required scope and prompt the user (this requires the user interacting with a web browser).
<<>>=
if(is.null(newResult)) {
  initializeAuth(aAuth, scope = paste("write project", projId))
  requestAccessToken(aAuth)
}
@ 


Assuming that we have the proper permissions, we can inspect the newly created 
{\tt AppResults} instance associated with our project. 
<<>>=
listAppResults(newProj)
@ 

Please note that at this stage the status of the {\tt AppResults} instance is set 
to {\it Running}.

\subsection{Uploading data/files}
\label{subsec:uploadfiles}

The function used for data/file uploads is {\tt putFiles()}. At the moment this function
implements only the POST method. Multiple file uploads will be soon supported using 
the same interface.

To upload the PNG file we need to specify the {\tt AppResults} ID and the file location
on the disk.

<<>>=
newFile <- putFiles(aAuth, resultId = Id(newResult), fIn = gfile)
@ 

If the above command succeeds, the file is uploaded to BaseSpace and a new {\tt Files}
instance is created for this file.
<<>>=
newFile
@ 

We can check the {\tt UploadStatus} given by the {\tt newFile} and if this is set to 
{\it Complete}, we can then finalize the session.

<<eval = FALSE>>=
if(newFile$UploadStatus != "complete")
  stop("Problem upload the result ...")
  
## delete the PNG file from the local storage
unlink(gfile)
@ 

%% we don't really do delete the figure, since we need it for generating the document!
<<echo = FALSE>>=
if(newFile$UploadStatus != "complete")
  stop("Problem upload the result ...")
@ 

To finalize the session we use the {\tt updateAppSessions()} method to set the 
status of the {\tt AppSessions} to {\it Complete}.

<<>>=
## Get the session Id
sessionId <- Id(AppSessions(newResult))

## Complete the session
invisible(updateAppSessions(aAuth, id = sessionId, status = "complete"))
@ 

If updating the {\tt AppSessions} is successful the user will receive a confirmation e-mail. 

We can further inspect the {\tt AppResults} to check if the status is properly set.
<<>>=
myResults <- listAppResults(newProj)
myResults
Status(myResults)
@ 



\section{Functions used}
\label{sec:functions}
Bellow are the functions used for computing the Q-score statistics in
Section~\ref{sec:crunching}. We use the {\tt readFastq()} function to read the 
FASTQ files and {\tt quality()} to extract the base qualities.

<<eval = FALSE>>=
getQscoreCounts <- function(fIn, maxS = 42L) {
  ## read the fastq file and keep the qualities 
  r <- quality(quality(readFastq(fIn, withIds = FALSE)))
  r_row <- width(r)[[1L]]
  r_col <- length(r)

  ## transorm the qualities to integers from 2 to maxS
  x <- as.integer(unlist(r, use.names = FALSE)) - 33L
  dim(x) <- c(r_row, r_col)

  ## tabulate each row in the matrix
  qtab <- matrix(0L, nrow = maxS, ncol = r_row,
                 dimnames = list(paste0("Q", seq_len(maxS)), NULL))
  for(i in seq_len(r_row)) 
    qtab[, i] <- tabulate(x[i, ], nbins = maxS)
  
  return(qtab)
}
@ 


<<eval = FALSE>>=
getQscoreStats <- function(x) {
  nr <- nrow(x)
  nc <- ncol(x)
  qstat <- matrix(0L, nrow = nc, ncol = 4L,
                  dimnames = list(NULL, c("5%", "median", "95%", "mean")))
  rleval <- seq_len(nr)
  for(i in seq_len(nc)) {
    r <- Rle(values = rleval, lengths = x[, i])
    qstat[i, ] <- c(quantile(r, probs = c(0.05, .5, 0.95)), mean(r))
  }

  return(qstat)
}
@ 



\section{Session Information}

The version number of R and packages loaded for generating the vignette were:

<<echo=FALSE,results=tex>>=
toLatex(sessionInfo())
@


\end{document}
